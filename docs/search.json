[
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "Modeling",
    "section": "",
    "text": "The purpose of modeling here is to refine the 8 original predictor set to improve model performance given EDA findings. Based off the EDA findings, we will be dropping the PhysActivity and HvyAlcoholConsump predictors as had definitions that were too extreme which made them less valuable than they could of been as the data was quite misleading specifically with PhysActivity as it only required you exercise once outside of your job in the last 30 days. Even exercising once in 30 days is considered quite a severe sedentary lifestyle and would still contribute to serious health consequences so that predictor could not be used. HvyAlcoholConsump also had a too of an extreme definition as 14 drinks for males and 7 for females per week is quite extreme where majority of people do not reach that high of a threshold but may be a bit lower. With this strict and extreme definition, those people who still drink a good amount but not up to the extreme definition would not be captured by this variable. After the removal of these 2 predictors, we will build different predictive models based off the refined feature set such as logistic regression, classification trees, and random forest models for this task. Our goal is to identify the most effective model for predicting the binary outcome variable, “Diabetes”, using a training/test split of the data along with 5 fold cross validation and then evaluating the models’ performances based on log loss and selecting the best one. Log loss measures the performance of a binary classification model by measuring the accuracy of its predicted probabilities. Accuracy only counts the number of correct predictions, but log loss takes into account the confidence of these predictions as well. This means that log loss will penalize models more heavily for being confidently wrong. Considering these predicted probabilities instead of solely final classifications provides a more detailed assessment of model performance. This makes it quite valuable in situations where the confidence of predictions is crucial for decision-making (like a situation in diagnosing someone with a disease if they have it or not).\n\n# Load in required packages\nlibrary(caret)\nlibrary(readr)\nlibrary(dplyr)\n# Load in dataset\ndat &lt;- read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")\n# Factor conversion for response and selected predictors that are categorical\ndat$Diabetes_binary &lt;- factor(dat$Diabetes_binary, levels = c(0, 1), labels = c(\"No.Diabetes\", \"Diabetes\"))\ndat$HighBP &lt;- factor(dat$HighBP, levels = c(0, 1), labels = c(\"No High BP\", \"High BP\"))\ndat$HighChol &lt;- factor(dat$HighChol, levels = c(0, 1), labels = c(\"No High Cholesterol\", \"High Cholesterol\"))\ndat$Smoker &lt;- factor(dat$Smoker, levels = c(0, 1), labels = c(\"Non-Smoker\", \"Smoker\"))\ndat$HvyAlcoholConsump &lt;- factor(dat$HvyAlcoholConsump, levels = c(0, 1), labels = c(\"Non-Heavy Drinker\", \"Heavy Drinker\"))\ndat$PhysActivity &lt;- factor(dat$PhysActivity, levels = c(0, 1), labels = c(\"Has Not Exercised Outside of Work\", \"Has Exercised Outside of Work\"))\ndat$Sex &lt;- factor(dat$Sex, levels = c(0, 1), labels = c(\"Female\", \"Male\"))\ndat$Age &lt;- factor(dat$Age, \n                       levels = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13), \n                       labels = c(\"18-24\", \"25-29\", \"30-34\", \"35-39\", \"40-44\", \n                                  \"45-49\", \"50-54\", \"55-59\", \"60-64\", \"65-69\", \n                                  \"70-74\", \"75-79\", \"80 or older\"))\n#Rename response variable for simplicity\ndat &lt;- dat |&gt; rename(Diabetes = Diabetes_binary)\ncols_to_keep &lt;- c(\"Diabetes\", \"HighBP\", \"HighChol\", \"BMI\", \"Smoker\", \"PhysActivity\", \"HvyAlcoholConsump\", \"Sex\", \"Age\")\ndat &lt;- dat[,cols_to_keep]\n# Setting the seed to make things reproducible\nset.seed(123)\n# Splitting the data into training and test sets\ntrainIndex &lt;- createDataPartition(dat$Diabetes, p = 0.7, list = FALSE)\ntrainData &lt;- dat[trainIndex, ]\ntestData &lt;- dat[-trainIndex, ]"
  },
  {
    "objectID": "Modeling.html#introduction",
    "href": "Modeling.html#introduction",
    "title": "Modeling",
    "section": "",
    "text": "The purpose of modeling here is to refine the 8 original predictor set to improve model performance given EDA findings. Based off the EDA findings, we will be dropping the PhysActivity and HvyAlcoholConsump predictors as had definitions that were too extreme which made them less valuable than they could of been as the data was quite misleading specifically with PhysActivity as it only required you exercise once outside of your job in the last 30 days. Even exercising once in 30 days is considered quite a severe sedentary lifestyle and would still contribute to serious health consequences so that predictor could not be used. HvyAlcoholConsump also had a too of an extreme definition as 14 drinks for males and 7 for females per week is quite extreme where majority of people do not reach that high of a threshold but may be a bit lower. With this strict and extreme definition, those people who still drink a good amount but not up to the extreme definition would not be captured by this variable. After the removal of these 2 predictors, we will build different predictive models based off the refined feature set such as logistic regression, classification trees, and random forest models for this task. Our goal is to identify the most effective model for predicting the binary outcome variable, “Diabetes”, using a training/test split of the data along with 5 fold cross validation and then evaluating the models’ performances based on log loss and selecting the best one. Log loss measures the performance of a binary classification model by measuring the accuracy of its predicted probabilities. Accuracy only counts the number of correct predictions, but log loss takes into account the confidence of these predictions as well. This means that log loss will penalize models more heavily for being confidently wrong. Considering these predicted probabilities instead of solely final classifications provides a more detailed assessment of model performance. This makes it quite valuable in situations where the confidence of predictions is crucial for decision-making (like a situation in diagnosing someone with a disease if they have it or not).\n\n# Load in required packages\nlibrary(caret)\nlibrary(readr)\nlibrary(dplyr)\n# Load in dataset\ndat &lt;- read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")\n# Factor conversion for response and selected predictors that are categorical\ndat$Diabetes_binary &lt;- factor(dat$Diabetes_binary, levels = c(0, 1), labels = c(\"No.Diabetes\", \"Diabetes\"))\ndat$HighBP &lt;- factor(dat$HighBP, levels = c(0, 1), labels = c(\"No High BP\", \"High BP\"))\ndat$HighChol &lt;- factor(dat$HighChol, levels = c(0, 1), labels = c(\"No High Cholesterol\", \"High Cholesterol\"))\ndat$Smoker &lt;- factor(dat$Smoker, levels = c(0, 1), labels = c(\"Non-Smoker\", \"Smoker\"))\ndat$HvyAlcoholConsump &lt;- factor(dat$HvyAlcoholConsump, levels = c(0, 1), labels = c(\"Non-Heavy Drinker\", \"Heavy Drinker\"))\ndat$PhysActivity &lt;- factor(dat$PhysActivity, levels = c(0, 1), labels = c(\"Has Not Exercised Outside of Work\", \"Has Exercised Outside of Work\"))\ndat$Sex &lt;- factor(dat$Sex, levels = c(0, 1), labels = c(\"Female\", \"Male\"))\ndat$Age &lt;- factor(dat$Age, \n                       levels = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13), \n                       labels = c(\"18-24\", \"25-29\", \"30-34\", \"35-39\", \"40-44\", \n                                  \"45-49\", \"50-54\", \"55-59\", \"60-64\", \"65-69\", \n                                  \"70-74\", \"75-79\", \"80 or older\"))\n#Rename response variable for simplicity\ndat &lt;- dat |&gt; rename(Diabetes = Diabetes_binary)\ncols_to_keep &lt;- c(\"Diabetes\", \"HighBP\", \"HighChol\", \"BMI\", \"Smoker\", \"PhysActivity\", \"HvyAlcoholConsump\", \"Sex\", \"Age\")\ndat &lt;- dat[,cols_to_keep]\n# Setting the seed to make things reproducible\nset.seed(123)\n# Splitting the data into training and test sets\ntrainIndex &lt;- createDataPartition(dat$Diabetes, p = 0.7, list = FALSE)\ntrainData &lt;- dat[trainIndex, ]\ntestData &lt;- dat[-trainIndex, ]"
  },
  {
    "objectID": "Modeling.html#logistic-regression-models",
    "href": "Modeling.html#logistic-regression-models",
    "title": "Modeling",
    "section": "Logistic Regression Models",
    "text": "Logistic Regression Models\n&gt; A logistic regression model is statistical method where the response variable is binary, so there are only 2 outcomes. The goal is to model the probability of a given input belonging to a certain category. Logistic regression estimates the probability of an event occurring by applying the logistic function. This function maps any real-valued number into a range between 0 and 1, representing the probability of the binary outcome. The model is expressed as a linear combination of predictor variables, and the coefficients indicate the change in the log odds of the outcome for a one-unit change in each predictor. We apply a logistic regression model to this kind of data because the response variable we are dealing with whether someone has diabetes or not is a binary variable which satisfies the main requirement to use logistic regression.\n\n# Define the control for cross-validation\ntrain_control &lt;- trainControl(method = \"cv\", number = 5, summaryFunction = mnLogLoss, classProbs = TRUE)\n\n# Fit logistic regression models with different predictor sets\n# Full model\nlogistic_model1 &lt;- train(Diabetes ~ HighBP + HighChol + BMI + Smoker + Sex + Age,\n                         data = trainData,\n                         method = \"glm\",\n                         family = \"binomial\",\n                         trControl = train_control,\n                         metric = \"logLoss\")\n# Model without Sex and Smoker\nlogistic_model2 &lt;- train(Diabetes ~ HighBP + HighChol + BMI + Age,\n                         data = trainData,\n                         method = \"glm\",\n                         family = \"binomial\",\n                         trControl = train_control,\n                         metric = \"logLoss\")\n# Model without Sex, Smoker, and Age\nlogistic_model3 &lt;- train(Diabetes ~ HighBP + HighChol + BMI,\n                         data = trainData,\n                         method = \"glm\",\n                         family = \"binomial\",\n                         trControl = train_control,\n                         metric = \"logLoss\")\n\n# Compare models\nresamples &lt;- resamples(list(Model1 = logistic_model1, Model2 = logistic_model2, Model3 = logistic_model3))\nsummary(resamples)\n\n\nCall:\nsummary.resamples(object = resamples)\n\nModels: Model1, Model2, Model3 \nNumber of resamples: 5 \n\nlogLoss \n            Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nModel1 0.3372755 0.3400550 0.3402558 0.3403311 0.3403820 0.3436874    0\nModel2 0.3391115 0.3404175 0.3405589 0.3408505 0.3420350 0.3421295    0\nModel3 0.3474662 0.3482802 0.3496145 0.3492893 0.3501775 0.3509083    0"
  },
  {
    "objectID": "Modeling.html#classification-tree",
    "href": "Modeling.html#classification-tree",
    "title": "Modeling",
    "section": "Classification Tree",
    "text": "Classification Tree\n&gt; A classification tree is a type of decision tree used for classifying a dataset into distinct categories. It is made by recursively splitting the data based on variable values, with the aim of creating subsets that are as homogeneous as possible with respect to the response variable. It involves selecting the best feature at each node, based on criteria like Gini impurity or entropy, and continuing to make branches until a stopping criterion is met. The leaf nodes then represent the final class labels. We may try and use it here in this case as classification trees handle categorical and non-categorical predictors well. It also allows for clear and intuitive understanding of how different factors contribute to the risk of diabetes and provides insights into feature importance, showing us the key factors leading to the development of diabetes. Furthermore, it is made to handle a categorical response variable since its sole purpose is to classify, so it works for this dataset in this case.\n\n# Load in required packages\nlibrary(rpart)\nlibrary(rpart.plot)\n# Setting the seed to make things reproducible\nset.seed(123)\n# Define the control for cross-validation\ntrain_control &lt;- trainControl(method = \"cv\", \n                              number = 5, \n                              summaryFunction = mnLogLoss, \n                              classProbs = TRUE)\n\n# Define a grid for the complexity parameter (cp)\ntune_grid &lt;- expand.grid(cp = seq(0, 0.1, by = 0.001))\n\n# Train a classification tree model with varying cp values\ntree_model &lt;- train(Diabetes ~ HighBP + HighChol + BMI + Smoker + Sex + Age,\n                    data = trainData,\n                    method = \"rpart\",\n                    trControl = train_control,\n                    tuneGrid = tune_grid,\n                    metric = \"logLoss\")\n\n# Print the best model and its parameters\nprint(tree_model)\n\nCART \n\n177577 samples\n     6 predictor\n     2 classes: 'No.Diabetes', 'Diabetes' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142061, 142062, 142061, 142062, 142062 \nResampling results across tuning parameters:\n\n  cp     logLoss  \n  0.000  0.3515673\n  0.001  0.3855362\n  0.002  0.4037576\n  0.003  0.4037576\n  0.004  0.4037576\n  0.005  0.4037576\n  0.006  0.4037576\n  0.007  0.4037576\n  0.008  0.4037576\n  0.009  0.4037576\n  0.010  0.4037576\n  0.011  0.4037576\n  0.012  0.4037576\n  0.013  0.4037576\n  0.014  0.4037576\n  0.015  0.4037576\n  0.016  0.4037576\n  0.017  0.4037576\n  0.018  0.4037576\n  0.019  0.4037576\n  0.020  0.4037576\n  0.021  0.4037576\n  0.022  0.4037576\n  0.023  0.4037576\n  0.024  0.4037576\n  0.025  0.4037576\n  0.026  0.4037576\n  0.027  0.4037576\n  0.028  0.4037576\n  0.029  0.4037576\n  0.030  0.4037576\n  0.031  0.4037576\n  0.032  0.4037576\n  0.033  0.4037576\n  0.034  0.4037576\n  0.035  0.4037576\n  0.036  0.4037576\n  0.037  0.4037576\n  0.038  0.4037576\n  0.039  0.4037576\n  0.040  0.4037576\n  0.041  0.4037576\n  0.042  0.4037576\n  0.043  0.4037576\n  0.044  0.4037576\n  0.045  0.4037576\n  0.046  0.4037576\n  0.047  0.4037576\n  0.048  0.4037576\n  0.049  0.4037576\n  0.050  0.4037576\n  0.051  0.4037576\n  0.052  0.4037576\n  0.053  0.4037576\n  0.054  0.4037576\n  0.055  0.4037576\n  0.056  0.4037576\n  0.057  0.4037576\n  0.058  0.4037576\n  0.059  0.4037576\n  0.060  0.4037576\n  0.061  0.4037576\n  0.062  0.4037576\n  0.063  0.4037576\n  0.064  0.4037576\n  0.065  0.4037576\n  0.066  0.4037576\n  0.067  0.4037576\n  0.068  0.4037576\n  0.069  0.4037576\n  0.070  0.4037576\n  0.071  0.4037576\n  0.072  0.4037576\n  0.073  0.4037576\n  0.074  0.4037576\n  0.075  0.4037576\n  0.076  0.4037576\n  0.077  0.4037576\n  0.078  0.4037576\n  0.079  0.4037576\n  0.080  0.4037576\n  0.081  0.4037576\n  0.082  0.4037576\n  0.083  0.4037576\n  0.084  0.4037576\n  0.085  0.4037576\n  0.086  0.4037576\n  0.087  0.4037576\n  0.088  0.4037576\n  0.089  0.4037576\n  0.090  0.4037576\n  0.091  0.4037576\n  0.092  0.4037576\n  0.093  0.4037576\n  0.094  0.4037576\n  0.095  0.4037576\n  0.096  0.4037576\n  0.097  0.4037576\n  0.098  0.4037576\n  0.099  0.4037576\n  0.100  0.4037576\n\nlogLoss was used to select the optimal model using the smallest value.\nThe final value used for the model was cp = 0."
  },
  {
    "objectID": "Modeling.html#random-forest",
    "href": "Modeling.html#random-forest",
    "title": "Modeling",
    "section": "Random Forest",
    "text": "Random Forest\n&gt; A random forest is an ensemble learning method that builds multiple decision trees and combines their predictions to improve overall model performance and robustness. Unlike single decision trees, which can easily overfit to training data and may not generalize well to new and unseen data, a random forest minimizes overfitting by averaging the predictions of numerous trees, each trained on different subsets of data along with features which makes sure all aspects of the data are captured. Going over the general process, it begins with bootstrap sampling, where multiple subsets of the training data are created through random sampling with replacement. For each subset, a decision tree is trained using feature randomization, where a random subset of features is considered for each node split, reducing the correlation between the trees and preventing overfitting. Once the trees are trained, the model combines predictions from all the trees and makes a final prediction which is done through majority voting. This ensemble approach compared to the single classification tree approach enhances predictive accuracy and reduces variance, making it more reliable and stable. It also is able to handle noisy data and large datasets more effectively, providing a more accurate and generalized model while sacrificing some interpretability. This makes it a preferred choice for complex tasks where a single classification tree may not perform as well.\n\n# Load in required package\nlibrary(randomForest)\n# Setting the seed to make things reproducible\nset.seed(123)\n# Define the control for cross-validation\ntrain_control &lt;- trainControl(method = \"cv\", \n                              number = 3, \n                              summaryFunction = mnLogLoss, \n                              classProbs = TRUE)\n# Define a grid for the number of features (mtry) considered at each split\ntune_grid &lt;- expand.grid(mtry = c(1:5))\n# Train a random forest model\nrf_model &lt;- train(Diabetes ~ HighBP + HighChol + BMI + Smoker + Sex + Age,\n                  data = trainData,\n                  method = \"rf\",\n                  trControl = train_control,\n                  tuneGrid = tune_grid,\n                  metric = \"logLoss\",\n                  ntree = 100)  # Number of trees\n\n# Print the best model and its parameters\nprint(rf_model)\n\nRandom Forest \n\n177577 samples\n     6 predictor\n     2 classes: 'No.Diabetes', 'Diabetes' \n\nNo pre-processing\nResampling: Cross-Validated (3 fold) \nSummary of sample sizes: 118384, 118385, 118385 \nResampling results across tuning parameters:\n\n  mtry  logLoss \n  1     4.283108\n  2     3.822335\n  3     3.549724\n  4     3.385609\n  5     3.323102\n\nlogLoss was used to select the optimal model using the smallest value.\nThe final value used for the model was mtry = 5."
  },
  {
    "objectID": "Modeling.html#final-model-selection",
    "href": "Modeling.html#final-model-selection",
    "title": "Modeling",
    "section": "Final Model Selection",
    "text": "Final Model Selection\n\n# Ensure the test set has the same factor levels as the training data\ntestData$Diabetes &lt;- factor(testData$Diabetes, levels = levels(trainData$Diabetes))\n\n# Generate probability predictions for each model\n# Logistic Regression\nlogistic_pred &lt;- predict(logistic_model1, newdata = testData, type = \"prob\")[,2]  # Probability of \"Diabetes\"\n\n# Classification Tree\ntree_pred &lt;- predict(tree_model, newdata = testData, type = \"prob\")[,2]  # Probability of \"Diabetes\"\n\n# Random Forest\nrf_pred &lt;- predict(rf_model, newdata = testData, type = \"prob\")[,2]  # Probability of \"Diabetes\"\n\n# True values for the test set\ntrue_values &lt;- as.numeric(testData$Diabetes) - 1  # Convert to 0 and 1\n\n# Function to calculate Log Loss\nlog_loss &lt;- function(true_values, predictions) {\n  epsilon &lt;- 1e-15  # Small value to avoid log(0)\n  predictions &lt;- pmax(pmin(predictions, 1 - epsilon), epsilon)  # Clip predictions\n  -mean(true_values * log(predictions) + (1 - true_values) * log(1 - predictions))\n}\n\n# Calculate Log Loss for each model\nlogistic_log_loss &lt;- log_loss(true_values, logistic_pred)\ntree_log_loss &lt;- log_loss(true_values, tree_pred)\nrf_log_loss &lt;- log_loss(true_values, rf_pred)\n\n# Print Log Loss for each model\ncat(\"Logistic Regression Log Loss:\", logistic_log_loss, \"\\n\")\n\nLogistic Regression Log Loss: 0.339463 \n\ncat(\"Classification Tree Log Loss:\", tree_log_loss, \"\\n\")\n\nClassification Tree Log Loss: 0.348121 \n\ncat(\"Random Forest Log Loss:\", rf_log_loss, \"\\n\")\n\nRandom Forest Log Loss: 3.44657 \n\n# Compare models based on Log Loss\nlog_loss_results &lt;- data.frame(\n  Model = c(\"Logistic Regression\", \"Classification Tree\", \"Random Forest\"),\n  LogLoss = c(logistic_log_loss, tree_log_loss, rf_log_loss)\n)\n\n# Print results\nprint(log_loss_results)\n\n                Model  LogLoss\n1 Logistic Regression 0.339463\n2 Classification Tree 0.348121\n3       Random Forest 3.446570\n\n# Determine the best model\nbest_model &lt;- log_loss_results[which.min(log_loss_results$LogLoss), ]\ncat(\"The best model is:\", best_model$Model, \"with a Log Loss of\", best_model$LogLoss, \"\\n\")\n\nThe best model is: Logistic Regression with a Log Loss of 0.339463"
  },
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "In this exploratory analysis, we are going to examine the Diabetes Health Indicators dataset. This dataset has 253,680 observations in response to the CDC’s BRFSS2015 survey and includes various health indicators and demographic information, with the ultimate goal of understanding factors that contribute to diabetes. The main predictors that we will be working with to help predict if someone will have diabetes or not will be a binary variable HighBP (whether someone has high blood pressure or not), a binary variable HighCol (whether someone has high cholesterol or not), a discrete variable BMI (Body Mass Index), a binary variable Smoker (whether someone has smoked at least 100 cigarettes in their lifetime or not), a binary variable PhysActivity (whether someone has had physical activity in the last 30 days outside of their job or not), a binary variable HvyAlcoholConsump (whether someone is a heavy drinker or not so that is considered adult men having more than 14 drinks per week or adult women having more than 7 drinks per week), a binary variable Sex (male or female), and 13-level categorical variable Age (split up into 13 groups of different age ranges). Going into the process of selecting these 8 predictors from the 21 total predictors in the dataset, we wanted to minimize variables that were prone to subjectivity as much as we could and select variables that we know based off intuition would have a strong effect on a person having diabetes or not. This is why HighBP, HighCol and BMI were selected as these are factors known to strongly contribute to diabetes. Also, smoking, not exercising and heavy alcohol consumption are also know activities that lead to a plethora of health problems which is why they are included as well. Age and sex were selected just due to curiosity to see the prevalence of diabetes between sexes and age groups as there would likely be a trend with more of one sex having diabetes and more of an age group or a set of age groups having diabetes as well (ex. older people).\nThe purpose of this EDA is to validate our health-related intuition that was used in selecting a subset of predictors to further look into. This could be how strong their relationships are with the response or any patterns or anomalies that could affect the predictive power of each predictor. Also, we want to check for any missing values to ensure our data is prepared for modeling.\nThe ultimate goal of modeling is to first refine the 8 original predictor set if need be to improve model performance given EDA findings. Then, we will build different predictive models based off the refined feature set and pick the one that performs the best on a test set or unseen data. This will show which model can most accurately predict individuals who are at risk of diabetes based on a set of factors."
  },
  {
    "objectID": "EDA.html#introduction",
    "href": "EDA.html#introduction",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "In this exploratory analysis, we are going to examine the Diabetes Health Indicators dataset. This dataset has 253,680 observations in response to the CDC’s BRFSS2015 survey and includes various health indicators and demographic information, with the ultimate goal of understanding factors that contribute to diabetes. The main predictors that we will be working with to help predict if someone will have diabetes or not will be a binary variable HighBP (whether someone has high blood pressure or not), a binary variable HighCol (whether someone has high cholesterol or not), a discrete variable BMI (Body Mass Index), a binary variable Smoker (whether someone has smoked at least 100 cigarettes in their lifetime or not), a binary variable PhysActivity (whether someone has had physical activity in the last 30 days outside of their job or not), a binary variable HvyAlcoholConsump (whether someone is a heavy drinker or not so that is considered adult men having more than 14 drinks per week or adult women having more than 7 drinks per week), a binary variable Sex (male or female), and 13-level categorical variable Age (split up into 13 groups of different age ranges). Going into the process of selecting these 8 predictors from the 21 total predictors in the dataset, we wanted to minimize variables that were prone to subjectivity as much as we could and select variables that we know based off intuition would have a strong effect on a person having diabetes or not. This is why HighBP, HighCol and BMI were selected as these are factors known to strongly contribute to diabetes. Also, smoking, not exercising and heavy alcohol consumption are also know activities that lead to a plethora of health problems which is why they are included as well. Age and sex were selected just due to curiosity to see the prevalence of diabetes between sexes and age groups as there would likely be a trend with more of one sex having diabetes and more of an age group or a set of age groups having diabetes as well (ex. older people).\nThe purpose of this EDA is to validate our health-related intuition that was used in selecting a subset of predictors to further look into. This could be how strong their relationships are with the response or any patterns or anomalies that could affect the predictive power of each predictor. Also, we want to check for any missing values to ensure our data is prepared for modeling.\nThe ultimate goal of modeling is to first refine the 8 original predictor set if need be to improve model performance given EDA findings. Then, we will build different predictive models based off the refined feature set and pick the one that performs the best on a test set or unseen data. This will show which model can most accurately predict individuals who are at risk of diabetes based on a set of factors."
  },
  {
    "objectID": "EDA.html#data",
    "href": "EDA.html#data",
    "title": "Exploratory Data Analysis",
    "section": "Data",
    "text": "Data\n\n# Load in required packages\nlibrary(readr)\nlibrary(dplyr)\n# Load in dataset\ndat &lt;- read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")\n# Checking for missing values\nsum(is.na(dat))\n\n[1] 0\n\n# Factor conversion for response and selected predictors that are categorical\ndat$Diabetes_binary &lt;- factor(dat$Diabetes_binary, levels = c(0, 1), labels = c(\"No.Diabetes\", \"Diabetes\"))\ndat$HighBP &lt;- factor(dat$HighBP, levels = c(0, 1), labels = c(\"No High BP\", \"High BP\"))\ndat$HighChol &lt;- factor(dat$HighChol, levels = c(0, 1), labels = c(\"No High Cholesterol\", \"High Cholesterol\"))\ndat$Smoker &lt;- factor(dat$Smoker, levels = c(0, 1), labels = c(\"Non-Smoker\", \"Smoker\"))\ndat$HvyAlcoholConsump &lt;- factor(dat$HvyAlcoholConsump, levels = c(0, 1), labels = c(\"Non-Heavy Drinker\", \"Heavy Drinker\"))\ndat$PhysActivity &lt;- factor(dat$PhysActivity, levels = c(0, 1), labels = c(\"Has Not Exercised Outside of Work\", \"Has Exercised Outside of Work\"))\ndat$Sex &lt;- factor(dat$Sex, levels = c(0, 1), labels = c(\"Female\", \"Male\"))\ndat$Age &lt;- factor(dat$Age, \n                       levels = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13), \n                       labels = c(\"18-24\", \"25-29\", \"30-34\", \"35-39\", \"40-44\", \n                                  \"45-49\", \"50-54\", \"55-59\", \"60-64\", \"65-69\", \n                                  \"70-74\", \"75-79\", \"80 or older\"))\n#Rename response variable for simplicity\ndat &lt;- dat |&gt; rename(Diabetes = Diabetes_binary)\ncols_to_keep &lt;- c(\"Diabetes\", \"HighBP\", \"HighChol\", \"BMI\", \"Smoker\", \"PhysActivity\", \"HvyAlcoholConsump\", \"Sex\", \"Age\")\ndat &lt;- dat[,cols_to_keep]"
  },
  {
    "objectID": "EDA.html#summarizations",
    "href": "EDA.html#summarizations",
    "title": "Exploratory Data Analysis",
    "section": "Summarizations",
    "text": "Summarizations\nTo take a closer look at our predictors and their relationships with the response variable, we created numerical and visual summaries for each predictor. Looking at the numerical summaries (contingency tables) of the categorical predictors, we see that about 3/4 of the people observed here that have diabetes also have high blood pressure. This ratio though is not as strong when looking at high cholesterol as only 2/3 of subjects that have diabetes also have high cholesterol which is interesting to note as all patients that have diabetes may have only high blood pressure, and not always both. Looking at smoking, only about half of subjects with diabetes smoke as well. This is interesting as smoking may not have as big of an influence as we first thought. Moving onto PhysActivity, only about 37% of people with diabetes have not exercised out of work. This variable is a bit misleading though as it says exercises at least once in the past 30 days outside of work. That means subjects could have only exercised once in 30 days but that would still count for this variable which really hurts the value in the usage of this predictor. Looking at HvyAlcoholConsump, less than 5% of the people with diabetes are considered heavy drinkers, so this pretty much eliminates any usage of this predictor in the upcoming models. The reason for extremely low value could be the extreme definition of a heavy drinker being 14 drinks for males and 7 drinkers for females per week which is quite high on a week to week basis. The more likely scenario is there are a bunch of people that drink a bit less than this, but cannot be captured by the definition of this predictor. Looking at Sex, there are a bit more females with diabetes amounting to 2000 more, but there are also almost 30000 more females in the study than males. This shows how much more common diabetes is in males than females. For age, it looks like in the younger age groups like 18-24 and 25-29, less than 2% of participants have diabetes. Moving to a middle age group like 45-49, this percentage increases to around 9%. For the older age groups like 75-79 and 80+, this percentage increases to around 20%. This shows how diabetes certainly becomes more common as you age. Looking at the numeric summary of BMI, a median BMI of 27 shows that we are dealing with a slightly overweight sample which is important to take into account as a higher BMI can be caused by or a side effect of other lifestyle factors present in this dataset. Looking at the boxplot for BMI comparing people with and without diabetes, the people with diabetes certainly have a higher average BMI than those that do not as the BMI of the 75th percentile of people without diabetes is approximately equal to the BMI of the 50th percentile of people with diabetes. This is again emphasized in the overlapping histgrams for BMI as the no diabetes groups is certainly more right skewed than the diabetes group as the individuals with no diabetes tend to have lower BMI’s generally. The barplots show a visual representation of the contingency tables again remphasizing the relationships observed above. This can be very clearly seen in the age barplot as the 35-39 and 75-79 age groups have roughly the same number of people without diabetes, but the 75-79 age group has a substantially higher amount of people with diabetes. This again reinforces the fact that age plays a signficant role in determining if someone has diabetes or not especially in the extreme groups.\n\n# Load in required packages\nlibrary(ggplot2)\n# Check structure of data\nstr(dat)\n\ntibble [253,680 × 9] (S3: tbl_df/tbl/data.frame)\n $ Diabetes         : Factor w/ 2 levels \"No.Diabetes\",..: 1 1 1 1 1 1 1 1 2 1 ...\n $ HighBP           : Factor w/ 2 levels \"No High BP\",\"High BP\": 2 1 2 2 2 2 2 2 2 1 ...\n $ HighChol         : Factor w/ 2 levels \"No High Cholesterol\",..: 2 1 2 1 2 2 1 2 2 1 ...\n $ BMI              : num [1:253680] 40 25 28 27 24 25 30 25 30 24 ...\n $ Smoker           : Factor w/ 2 levels \"Non-Smoker\",\"Smoker\": 2 2 1 1 1 2 2 2 2 1 ...\n $ PhysActivity     : Factor w/ 2 levels \"Has Not Exercised Outside of Work\",..: 1 2 1 2 2 2 1 2 1 1 ...\n $ HvyAlcoholConsump: Factor w/ 2 levels \"Non-Heavy Drinker\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ Sex              : Factor w/ 2 levels \"Female\",\"Male\": 1 1 1 1 1 2 1 1 1 2 ...\n $ Age              : Factor w/ 13 levels \"18-24\",\"25-29\",..: 9 7 9 11 11 10 9 11 9 8 ...\n\n# Check basic numerical summaries of non-categorical variables and frequencies in each level for categorical variables\nsummary(dat)\n\n        Diabetes             HighBP                      HighChol     \n No.Diabetes:218334   No High BP:144851   No High Cholesterol:146089  \n Diabetes   : 35346   High BP   :108829   High Cholesterol   :107591  \n                                                                      \n                                                                      \n                                                                      \n                                                                      \n                                                                      \n      BMI               Smoker                                  PhysActivity   \n Min.   :12.00   Non-Smoker:141257   Has Not Exercised Outside of Work: 61760  \n 1st Qu.:24.00   Smoker    :112423   Has Exercised Outside of Work    :191920  \n Median :27.00                                                                 \n Mean   :28.38                                                                 \n 3rd Qu.:31.00                                                                 \n Max.   :98.00                                                                 \n                                                                               \n         HvyAlcoholConsump      Sex              Age       \n Non-Heavy Drinker:239424   Female:141974   60-64  :33244  \n Heavy Drinker    : 14256   Male  :111706   65-69  :32194  \n                                            55-59  :30832  \n                                            50-54  :26314  \n                                            70-74  :23533  \n                                            45-49  :19819  \n                                            (Other):87744  \n\n# Contingency tables of each categorical predictor with the response to see how they relate and interact\n# List of categorical predictor variables\npredictor_vars &lt;- c(\"HighBP\", \"HighChol\", \"Smoker\", \"PhysActivity\", \"HvyAlcoholConsump\", \"Sex\", \"Age\")\n# Loop through each variable to create and display contingency tables\nfor (var in predictor_vars) {\n  # Create the contingency table\n  cat(\"Table for Diabetes vs\", var, \"\\n\")\n  print(table(dat$Diabetes, dat[[var]]))\n  cat(\"\\n\")  # Add a blank line for readability\n}\n\nTable for Diabetes vs HighBP \n             \n              No High BP High BP\n  No.Diabetes     136109   82225\n  Diabetes          8742   26604\n\nTable for Diabetes vs HighChol \n             \n              No High Cholesterol High Cholesterol\n  No.Diabetes              134429            83905\n  Diabetes                  11660            23686\n\nTable for Diabetes vs Smoker \n             \n              Non-Smoker Smoker\n  No.Diabetes     124228  94106\n  Diabetes         17029  18317\n\nTable for Diabetes vs PhysActivity \n             \n              Has Not Exercised Outside of Work Has Exercised Outside of Work\n  No.Diabetes                             48701                        169633\n  Diabetes                                13059                         22287\n\nTable for Diabetes vs HvyAlcoholConsump \n             \n              Non-Heavy Drinker Heavy Drinker\n  No.Diabetes            204910         13424\n  Diabetes                34514           832\n\nTable for Diabetes vs Sex \n             \n              Female   Male\n  No.Diabetes 123563  94771\n  Diabetes     18411  16935\n\nTable for Diabetes vs Age \n             \n              18-24 25-29 30-34 35-39 40-44 45-49 50-54 55-59 60-64 65-69 70-74\n  No.Diabetes  5622  7458 10809 13197 15106 18077 23226 26569 27511 25636 18392\n  Diabetes       78   140   314   626  1051  1742  3088  4263  5733  6558  5141\n             \n              75-79 80 or older\n  No.Diabetes 12577       14154\n  Diabetes     3403        3209\n\n# Numerical summary of BMI\nsummary(dat$BMI)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  12.00   24.00   27.00   28.38   31.00   98.00 \n\n# Box plot of BMI distribution for people with and w/o diabetes\nggplot(dat, aes(x = Diabetes, y = BMI, fill = Diabetes)) +\n  geom_boxplot() +\n  labs(title = \"BMI Distribution by Diabetes Status\", x = \"Diabetes Status\", y = \"BMI\")\n\n\n\n\n\n\n\n# Overlayed Histogram as another way to compare BMI by Diabetes Status\nggplot(dat, aes(x = BMI, fill = Diabetes, color = Diabetes)) +\n  geom_histogram(position = \"identity\", binwidth = 1, alpha = 0.5, bins = 30) +\n  labs(title = \"Histogram of BMI by Diabetes Status\", x = \"BMI\", y = \"Count\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"lightblue\", \"salmon\")) +\n  scale_color_manual(values = c(\"darkblue\", \"darkred\")) +\n  theme(legend.title = element_blank())\n\n\n\n\n\n\n\n# Loop through each variable to create and display bar plots\nfor (var in predictor_vars) {\n  # Create the bar plot\n  p &lt;- ggplot(dat, aes_string(x = var, fill = \"Diabetes\")) +\n    geom_bar(position = \"dodge\", color = \"black\") +\n    labs(title = paste(\"Bar Plot of\", var, \"by Diabetes Status\"),\n         x = var,\n         y = \"Count\") +\n    scale_fill_manual(values = c(\"lightblue\", \"salmon\")) +\n    theme_minimal() +\n    theme(legend.title = element_blank())\n  \n  # Print the plot\n  print(p)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick here for the Modeling Page"
  }
]